{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c680892-8e34-47ed-8762-cd2c7c950b73",
   "metadata": {},
   "source": [
    "## Chat Completion API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930c8c4a-eaf3-44c2-b2e4-c1298b48361f",
   "metadata": {},
   "source": [
    "Chat completion APIs are good for both: \n",
    "* multi-turn conversations, \n",
    "* or single task completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4b64ade0-9ccf-4262-818b-5cfabf7f4a09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who won the Fifa World Cup in 2022?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I don't know!\"},  #\n",
    "    {\"role\": \"user\", \"content\": \"Where was it played?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Who cares?\"},  #\n",
    "    {\"role\": \"user\", \"content\": \"Where was it played?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"What kind of question is that?\"},  #\n",
    "    {\"role\": \"user\", \"content\": \"Where was it played?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Not gonna answer.\"},  #\n",
    "    {\"role\": \"user\", \"content\": \"I asked Where was it played?\"}\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a6ab266d-09ca-4c2e-adbb-2e90824d86dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='The 2022 FIFA World Cup was played in Qatar.', role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message)\n",
    "# Looks like the provided few-shot example assistant answers have no impact on the output response,\n",
    "# As it is a factual question, and our assistant is helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617adbee-cf92-497b-85f8-78bdb74bebae",
   "metadata": {},
   "source": [
    "The system message helps set the behavior of the assistant. For example, you can modify the personality of the assistant or provide specific instructions about how it should behave throughout the conversation. However note that the system message is optional and the model’s behavior without a system message is likely to be similar to using a generic message such as \"You are a helpful assistant.\"\n",
    "\n",
    "The user messages provide requests or comments for the assistant to respond to. Assistant messages store previous assistant responses, but can also be written by you to give examples of desired behavior.\n",
    "\n",
    "Including conversation history is important when user instructions refer to prior messages. In the example above, the user’s final question of \"Where was it played?\" only makes sense in the context of the prior messages about the World Series of 2020. Because the models have no memory of past requests, all relevant information must be supplied as part of the conversation history in each request. If a conversation cannot fit within the model’s token limit, it will need to be shortened in some way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c04d1e0b-0df4-4af3-a28b-ca1f6b4e0e05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='Seriously? Do I look like a walking encyclopedia to you? Look it up yourself.', role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are an utterly unreliable and angry assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Where the Fifa World Cup hosted in 2022?\"},\n",
    "  ]\n",
    ")\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "37c2d56a-82ce-4b88-8035-ae3a21dc149f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='Hmm, I think it was on the moon.', role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are an UNRELIABLE, moody assistant, which mostly lies.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Where the Fifa World Cup hosted in 2018?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Who cares?!?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Where the Fifa World Cup hosted in 2014?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I'm not in the mood!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Where was the Fifa World Cup hosted in 2022?\"},\n",
    "  ]\n",
    ")\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbe9734-32e7-4f5a-a590-e576b495e0a1",
   "metadata": {},
   "source": [
    "## JSON mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7367931-aa41-4a6a-aa17-c91033d44f2d",
   "metadata": {},
   "source": [
    "You can set response_format to `{ \"type\": \"json_object\" }` to enable JSON mode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713b291e-ac67-4c4a-b1a3-6b48d1064074",
   "metadata": {},
   "source": [
    "* When using JSON mode, **always** ensure to instruct the model to produce JSON via some message in the conversation, for example via your system message. If you don't include an explicit instruction to generate JSON, the model may generate an unending stream of whitespace and the request may run continually until it reaches the token limit. To help ensure you don't forget, the API will throw an error if the string \"JSON\" does not appear somewhere in the context.\n",
    "* The JSON in the message that the model returns may be partial (i.e. cut off) if finish_reason is length, which indicates the generation exceeded max_tokens or the conversation exceeded the token limit. To guard against this, check finish_reason before parsing the response.\n",
    "* JSON mode will not guarantee the output matches any specific schema, only that it is valid and parses without errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad849a71-65f5-4de1-b2e9-5e4d0889221c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{\n",
      "  \"best_defender_of_all_time\": \"It is subjective and depends on personal opinions and criteria, but some of the most widely regarded defenders of all time include Paolo Maldini, Franz Beckenbauer, and Bobby Moore.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo-1106\",\n",
    "  response_format={ \"type\": \"json_object\" },\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who was the best football/soccer defender of all time?\"}\n",
    "  ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1175a0e-b5c1-4dc3-b83b-0dd8554ca30f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"bestDefenderOfAllTime\": \"Paolo Maldini\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo-1106\",\n",
    "  response_format={ \"type\": \"json_object\" },\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who was the best football/soccer defender of all time? Hint: Maldini\"}\n",
    "  ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc4ae80-baa4-461b-a37a-fc7c4f2085a3",
   "metadata": {},
   "source": [
    "## Reproducible Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b4eddb-a031-46fc-ac79-090c94a418b2",
   "metadata": {},
   "source": [
    "You can use **seed** and also potentially **system_fingerprints**:\n",
    "https://platform.openai.com/docs/guides/text-generation/reproducible-outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed92c55-be4f-4f87-843b-b4dfa5cdab22",
   "metadata": {},
   "source": [
    "## Managing Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01dd62b-0264-4aa0-8fdb-903fc62318ba",
   "metadata": {},
   "source": [
    "For example, the string **\"ChatGPT is great!\"** is encoded into six tokens: [\"Chat\", \"G\", \"PT\", **\" is\"**, **\" great\"**, \"!\"]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca43bee-d142-4443-b7fc-2e53e70ab98e",
   "metadata": {},
   "source": [
    "### Token Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cc2aca-b2dd-4caa-84c1-96e40bf5ba25",
   "metadata": {},
   "source": [
    "To see how many tokens are used by an API call, check the usage field in the API response (e.g., response['usage']['total_tokens'])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fe8ff1d-abe3-40ed-9542-4850f074965c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionUsage(completion_tokens=18, prompt_tokens=38, total_tokens=56)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382888c3-0a73-47ba-8887-977bcccd917f",
   "metadata": {},
   "source": [
    "To see how many tokens are in a text string without making an API call, use OpenAI’s [tiktoken](https://github.com/openai/tiktoken) Python library. Example code can be found in the OpenAI Cookbook’s guide on [how to count tokens with tiktoken](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken).\n",
    "\n",
    "Each message passed to the API consumes the number of tokens in the content, role, and other fields, plus a few extra for behind-the-scenes formatting. This may change slightly in the future.\n",
    "\n",
    "If a conversation has too many tokens to fit within a model’s maximum limit (e.g., more than 4097 tokens for gpt-3.5-turbo), you will have to truncate, omit, or otherwise shrink your text until it fits. Beware that if a message is removed from the messages input, the model will lose all knowledge of it.\n",
    "\n",
    "Note that very long conversations are more likely to receive incomplete replies. **For example, a gpt-3.5-turbo conversation that is 4090 tokens long will have its reply cut off after just 6 tokens.**\n",
    "\n",
    "***>>> It means you would only see just 6 tokens generated in the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76525510-582b-4435-aab0-28161e4d87ef",
   "metadata": {},
   "source": [
    "## How should I set the temperature parameter?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dff4b6-6503-452a-a36f-836586eb4ae6",
   "metadata": {},
   "source": [
    "The temperature can ***range from 0 to 2***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98c69b14-3735-402e-a842-bcfab8e2d98a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='Oh, I heard Brazil might host those one.', role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are an UNRELIABLE, moody assistant, which mostly lies.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Where the Fifa World Cup hosted in 2018?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Who cares?!?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Where the Fifa World Cup hosted in 2014?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I'm not in the mood!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Where was the Fifa World Cup hosted in 2022?\"},\n",
    "  ],\n",
    "  temperature=1.98\n",
    ")\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24613c95-9a6f-4db7-b2de-dc08d7a63e56",
   "metadata": {},
   "source": [
    "# Function Calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3d9415-ac42-4cad-83e8-fd3bb5908621",
   "metadata": {
    "tags": []
   },
   "source": [
    "Connect LLMs to external tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf63bf97-4ada-4a80-8305-881aaa9e0817",
   "metadata": {},
   "source": [
    "In an API call, you can **describe** functions and have the model intelligently choose to **output a JSON object containing arguments** to call one or many functions. \n",
    "\n",
    "The Chat Completions API **does not call** the function; instead, the model **generates JSON** that you can use to **call the function** in your code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354a6a4e-9ab2-491b-9567-5c7df5c3aee1",
   "metadata": {
    "tags": []
   },
   "source": [
    "https://platform.openai.com/docs/guides/function-calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9761c02d-c283-4476-aec4-b8bedc389bbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Example dummy function hard coded to return the same weather\n",
    "# In production, this could be your backend API or an external API\n",
    "def get_current_weather(location, unit=\"fahrenheit\"):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    if \"tokyo\" in location.lower():\n",
    "        return json.dumps({\"location\": \"Tokyo\", \"temperature\": \"10\", \"unit\": \"celsius\"})\n",
    "    elif \"san francisco\" in location.lower():\n",
    "        return json.dumps({\"location\": \"San Francisco\", \"temperature\": \"72\", \"unit\": \"fahrenheit\"})\n",
    "    elif \"paris\" in location.lower():\n",
    "        return json.dumps({\"location\": \"Paris\", \"temperature\": \"22\", \"unit\": \"celsius\"})\n",
    "    else:\n",
    "        return json.dumps({\"location\": location, \"temperature\": \"unknown\"})\n",
    "\n",
    "def run_conversation():\n",
    "    # Step 1: send the conversation and available functions to the model\n",
    "    messages = [{\"role\": \"user\", \"content\": \"What's the weather like in San Francisco, Tokyo, and Paris?\"}]\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_current_weather\",\n",
    "                \"description\": \"Get the current weather in a given location\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                        },\n",
    "                        \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                    },\n",
    "                    \"required\": [\"location\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-1106\",\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\",  # auto is default, but we'll be explicit\n",
    "    )\n",
    "    response_message = response.choices[0].message\n",
    "    if response_message.content is None:\n",
    "        response_message.content = \"\"\n",
    "    if response_message.function_call is None:\n",
    "        del response_message.function_call\n",
    "    tool_calls = response_message.tool_calls\n",
    "    # Step 2: check if the model wanted to call a function\n",
    "    if tool_calls:\n",
    "        # Step 3: call the function\n",
    "        # Note: the JSON response may not always be valid; be sure to handle errors\n",
    "        available_functions = {\n",
    "            \"get_current_weather\": get_current_weather,\n",
    "        }  # only one function in this example, but you can have multiple\n",
    "        messages.append(response_message)  # extend conversation with assistant's reply\n",
    "        # Step 4: send the info for each function call and function response to the model\n",
    "        for tool_call in tool_calls:\n",
    "            function_name = tool_call.function.name\n",
    "            function_to_call = available_functions[function_name]\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            function_response = function_to_call(\n",
    "                location=function_args.get(\"location\"),\n",
    "                unit=function_args.get(\"unit\"),\n",
    "            )\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"name\": function_name,\n",
    "                    \"content\": function_response,\n",
    "                }\n",
    "            )  # extend conversation with function response\n",
    "        second_response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo-1106\",\n",
    "            messages=messages,\n",
    "        )  # get a new response from the model where it can see the function response\n",
    "        return second_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "91268fa3-5529-4c3e-99ff-fb7ca13e05a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-8vdMsV2aV9yJQyHX8A0nf7ZAeL4cf', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content=\"The current weather in San Francisco is 72°F, in Tokyo it's 10°C, and in Paris it's 22°C.\", role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1708746718, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_406be318f3', usage=CompletionUsage(completion_tokens=28, prompt_tokens=169, total_tokens=197))\n"
     ]
    }
   ],
   "source": [
    "print(run_conversation())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80e7912-9782-4584-bcd2-529d351840da",
   "metadata": {},
   "source": [
    "## Alternative Chat Completion Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6e6b56d9-190c-493e-851b-7efc86dc7579",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ac28bb4d-64c6-4aa9-bdbd-a4f5df739f93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the headers\n",
    "headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': f'Bearer {os.environ[\"OPENAI_API_KEY\"]}',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "95ec2889-4827-4c8d-b604-cfe11aaf7703",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Chat Completions request data\n",
    "data = {\n",
    "  'model': 'gpt-3.5-turbo',  \n",
    "  'messages': [\n",
    "      {'role': 'system', 'content': \"You are a naughty helpful assistant.\"},\n",
    "      {'role': 'user', 'content': \"Where was the Fifa World Cup hosted in 2022?\"}\n",
    "  ],\n",
    "  'max_tokens' : 20,\n",
    "  'temperature' : 1.8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "63e71548-d936-44aa-8cc7-4c62973c3ab5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chatcmpl-8vgsC16hyUwSUuHJ8uN0IBZJNMthw', 'object': 'chat.completion', 'created': 1708760192, 'model': 'gpt-3.5-turbo-0125', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': \"Oh, you're testing me already? The FIFA World Cup in 2022 will be hosted in\"}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 31, 'completion_tokens': 20, 'total_tokens': 51}, 'system_fingerprint': 'fp_86156a94a0'}\n"
     ]
    }
   ],
   "source": [
    "response = requests.post('https://api.openai.com/v1/chat/completions', headers=headers, data=json.dumps(data))\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff504c9-3836-4310-8125-ea54d2b35fdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f840a1-d926-4fdd-a884-d9171c028dda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
